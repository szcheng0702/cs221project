{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "#change it to your own path to where 'gym' is installed\n",
    "sys.path.append('/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages')\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import multi_gpu_model\n",
    "\n",
    "#from JSAnimation.IPython_display import display_animation\n",
    "from matplotlib import animation\n",
    "from IPython.display import display\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPISODES = 1000\n",
    "TIME_LIMIT = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import Sequence\n",
    "\n",
    "class DataSequence(Sequence):\n",
    "    def __init__(self, dataset, model, batch_size):\n",
    "        self.data=dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.model=model\n",
    "        self.gamma = 0.95 # discount rate\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.data) / float(self.batch_size)))\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):        \n",
    "        minibatch=self.data[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "\n",
    "        states=[]\n",
    "        targets_f=[]\n",
    "\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            states.append(state)\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = (reward + self.gamma *\n",
    "                          np.amax(self.model.predict(next_state)[0]))\n",
    "\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "\n",
    "            targets_f.append(target_f)\n",
    "        return np.array(states), np.array(targets_f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size,batch_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=5000) # maximum number of samples stored in dataset\n",
    "        self.gamma = 0.95 # discount rate\n",
    "        #self.epsilon = 0.2 # exploration rate\n",
    "        self.epsilon = 1.0 # exploration rate\n",
    "        self.epsilon_min = 0.01 # minimum exploration rate\n",
    "        self.epsilon_decay = 0.995 # decay rate for exploration\n",
    "        #self.epsilon_decay = 1 # decay rate for exploration\n",
    "        self.learning_rate = 0.1\n",
    "        self.batch_size=batch_size\n",
    "        self.model = self.normal_model(4)\n",
    "\n",
    "    def _build_model_2L(self):\n",
    "        \"\"\"2-layer Neural Net for Deep-Q learning Model.\"\"\"\n",
    "        model = Sequential()\n",
    "        model.add(Dense(units=24, input_dim=self.state_size, activation='relu')) # input layer\n",
    "        model.add(Dense(units=self.action_size, activation='linear')) # output layer\n",
    "        #model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate)) # loss function = mean squared error\n",
    "        return model\n",
    "    \n",
    "    def _build_model_3L(self):\n",
    "        \"\"\"3-layer Neural Net for Deep-Q learning Model.\"\"\"\n",
    "        model = Sequential()\n",
    "        model.add(Dense(units=24, input_dim=self.state_size, activation='relu')) # input layer\n",
    "        model.add(Dense(units=24, activation='relu'))\n",
    "        model.add(Dense(units=self.action_size, activation='linear')) # output layer\n",
    "        #model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate)) # loss function = mean squared error\n",
    "        return model\n",
    "\n",
    "    def _build_model_4L(self):\n",
    "        \"\"\"4-layer Neural Net for Deep-Q learning Model.\"\"\"\n",
    "        model = Sequential()\n",
    "        model.add(Dense(units=24, input_dim=self.state_size, activation='relu')) # input layer\n",
    "        model.add(Dense(units=24, activation='relu'))\n",
    "        model.add(Dense(units=24, activation='relu'))\n",
    "        model.add(Dense(units=self.action_size, activation='linear')) # output layer\n",
    "        #model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate)) # loss function = mean squared error\n",
    "        return model\n",
    "    \n",
    "    def normal_model(self,layer=4):\n",
    "        if layer==2:\n",
    "            model=self._build_model_2L()\n",
    "        elif layer==3:\n",
    "            model = self._build_model_3L()\n",
    "        elif layer==4:\n",
    "            model = self._build_model_4L()\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate)) # loss function = mean squared error\n",
    "        return model\n",
    "        \n",
    "    def parallel_model(self,layer=4):\n",
    "        if layer==2:\n",
    "            model=self._build_model_2L()\n",
    "        elif layer==3:\n",
    "            model = self._build_model_3L()\n",
    "        elif layer==4:\n",
    "            model = self._build_model_4L()\n",
    "        # Not needed to change the device scope for model definition:\n",
    "        p_model = multi_gpu_model(model, cpu_relocation=True)\n",
    "        p_model.compile(loss='mse',optimizer='Adam(lr=self.learning_rate)')\n",
    "        return p_model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store s,a,r,s' by appending to self.memory.\"\"\"\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"Choose action randomly (explore) or by model prediction (exploit).\"\"\"\n",
    "        if np.random.rand() <= self.epsilon: # explore with probabiluty self.epsilon\n",
    "            return random.randrange(self.action_size)\n",
    "\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])  # returns action\n",
    "    \n",
    "    def fit_model(self,minibatch,next_state):\n",
    "#         for state, action, reward, next_state, done in minibatch:\n",
    "#             target = reward\n",
    "#             if not done:\n",
    "#                 target = (reward + self.gamma *\n",
    "#                           np.amax(self.model.predict(next_state)[0]))\n",
    "\n",
    "#             target_f = self.model.predict(state)\n",
    "#             target_f[0][action] = target\n",
    "#         multiprocess_batchsize=500\n",
    "    \n",
    "        seq=DataSequence(minibatch,self.model,self.batch_size)\n",
    "        \n",
    "        self.model._make_predict_function()\n",
    "        graph = tf.get_default_graph()\n",
    "        #self.model.compile() \n",
    "\n",
    "    #Using the below code when calling model.predict() :\n",
    "        with graph.as_default():\n",
    "            preds = self.model.predict(next_state)\n",
    "            self.model.fit_generator(generator=seq, \n",
    "                epochs = 1, \n",
    "                verbose=1,\n",
    "                workers=8,\n",
    "                use_multiprocessing=True)\n",
    "        #self.model.fit(state, target_f, epochs=1, verbose=0) # epochs = number of iterations over the minibatch\n",
    "            \n",
    "    def replay(self, batch_size,next_state):\n",
    "        \"\"\"Train the neural net on the episodes in self.memory. \n",
    "           Only N samples defined by batch_size are sampled from self.memory for training.\n",
    "        \"\"\"\n",
    "        minibatch = random.sample(self.memory, len(self.memory))\n",
    "        #chunks = [minibatch[i:i + 100] for i in range(0, len(minibatch), 100)]\n",
    "        #pool=Pool()\n",
    "        #pool.map(self.fit_model, chunks)\n",
    "\n",
    "        self.fit_model(minibatch,next_state)\n",
    "              \n",
    "        # Decaying exploration rate\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('Phoenix-ram-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# state_size = env.observation_space.shape[0]\n",
    "# action_size = env.action_space.n\n",
    "# agent = DQNAgent(state_size, action_size)\n",
    "# done = False\n",
    "batch_size = 500\n",
    "# scores = [] # store the score for each completed episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "if __name__ == '__main__':\n",
    "    env = gym.make('Phoenix-ram-v0')\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "    agent = DQNAgent(state_size, action_size,batch_size)\n",
    "    done = False\n",
    "    #batch_size = 5000\n",
    "    scores = [] # store the score for each completed episode\n",
    "\n",
    "    for episode in range(EPISODES):\n",
    "        #start_time=time.clock()\n",
    "        \n",
    "        print('episode = {}'.format(episode))\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "        score = 0\n",
    "\n",
    "        for time in range(TIME_LIMIT):\n",
    "            # env.render()\n",
    "            action = agent.act(state) # DQN agent chooses next action \n",
    "            next_state, reward, done, _ = env.step(action) # observe rewards and successor state\n",
    "            score += reward # keep track of game score\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            agent.remember(state, action, reward, next_state, done) # add s,a,r,s' to dataset (agent.memory)\n",
    "            state = next_state        \n",
    "                    \n",
    "            if done or time==TIME_LIMIT:\n",
    "                print('episode: {}/{}, scores: {}, exploration rate: {:.2}'\n",
    "                          .format(episode, EPISODES, scores, agent.epsilon))\n",
    "                scores.append(score)\n",
    "                #print('scores = {}'.format(scores))\n",
    "                break\n",
    "\n",
    "            # Train NN after each episode or timeout by randomly sampling a batch from the dataset in agent.memory\n",
    "            if len(agent.memory) > batch_size:\n",
    "                agent.replay(batch_size,state)\n",
    "                \n",
    "        #print ('time elpased={}'.format(time.clock()-start_time))\n",
    "        \n",
    "        if episode%20==0: #save every 20 episodes\n",
    "            # Save weights after training is complete\n",
    "            agent.save('phoenix_dqn_3L.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate average score\n",
    "print('AVERAGE SCORE = {}'.format(np.mean(np.asarray(scores))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
